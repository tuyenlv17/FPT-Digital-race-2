{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import os\n",
    "from skimage import io as ski_io\n",
    "from skimage.viewer import ImageViewer\n",
    "import time\n",
    "from nolearn.lasagne import BatchIterator\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_path_prefix(c):\n",
    "    return \"0\" + str(c)\n",
    "def one_hot(labels, n_class = NUM_CLASS):\n",
    "    total_record = len(labels)\n",
    "    y = np.zeros((total_record, n_class))\n",
    "    y[np.arange(total_record), labels] = 1\n",
    "    return y\n",
    "\n",
    "def load_data():\n",
    "    X = []\n",
    "    y = []\n",
    "    path = './signs-gray-32x32'\n",
    "    for i in range(10):\n",
    "        class_path=path + \"/\" + add_path_prefix(i)\n",
    "        list_files=os.listdir(class_path)\n",
    "        for cur_file in list_files:\n",
    "            file_path = class_path + \"/\" + cur_file\n",
    "            img = ski_io.imread(file_path, as_grey=True)\n",
    "            img = (img / 255.).astype(np.float32)\n",
    "#             print (file_path)\n",
    "#             print (img)\n",
    "#             return \n",
    "            img = img.reshape(img.shape + (1,)) \n",
    "            X.append(img)\n",
    "            y.append(i)\n",
    "    y = one_hot(np.array(y))\n",
    "    return np.array(X, dtype=np.float32), y\n",
    "X_train, y_train = load_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2, random_state=12132)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.1, random_state=54651)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Parameters = namedtuple('Parameters', [\n",
    "        # Data parameters\n",
    "        'num_classes', 'image_size', \n",
    "        # Training parameters\n",
    "        'batch_size', 'max_epochs', 'log_epoch', 'print_epoch',\n",
    "        # Optimisations\n",
    "        'learning_rate_decay', 'learning_rate',\n",
    "        'l2_reg_enabled', 'l2_lambda', \n",
    "        'early_stopping_enabled', 'early_stopping_patience', \n",
    "        'resume_training', \n",
    "        # Layers architecture\n",
    "        'conv1_k', 'conv1_d', 'conv1_p', \n",
    "        'conv2_k', 'conv2_d', 'conv2_p', \n",
    "        'conv3_k', 'conv3_d', 'conv3_p', \n",
    "        'fc4_size', 'fc4_p'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(axis, params, train_column, valid_column, linewidth = 2, train_linestyle = \"b-\", valid_linestyle = \"g-\"):\n",
    "    \"\"\"\n",
    "    Plots a pair of validation and training curves on a single plot.\n",
    "    \"\"\"\n",
    "    model_history = np.load(Paths(params).train_history_path + \".npz\")\n",
    "    train_values = model_history[train_column]\n",
    "    valid_values = model_history[valid_column]\n",
    "    epochs = train_values.shape[0]\n",
    "    x_axis = np.arange(epochs)\n",
    "    axis.plot(x_axis[train_values > 0], train_values[train_values > 0], train_linestyle, linewidth=linewidth, label=\"train\")\n",
    "    axis.plot(x_axis[valid_values > 0], valid_values[valid_values > 0], valid_linestyle, linewidth=linewidth, label=\"valid\")\n",
    "    return epochs\n",
    "\n",
    "# Plots history of learning curves for a specific model.\n",
    "def plot_learning_curves(params):\n",
    "    \"\"\"\n",
    "    Plots learning curves (loss and accuracy on both training and validation sets) for a model identified by a parameters struct.\n",
    "    \"\"\"\n",
    "    curves_figure = pyplot.figure(figsize = (10, 4))\n",
    "    axis = curves_figure.add_subplot(1, 2, 1)\n",
    "    epochs_plotted = plot_curve(axis, parameters, train_column = \"train_accuracy_history\", valid_column = \"valid_accuracy_history\")\n",
    "\n",
    "    pyplot.grid()\n",
    "    pyplot.legend()\n",
    "    pyplot.xlabel(\"epoch\")\n",
    "    pyplot.ylabel(\"accuracy\")\n",
    "    pyplot.ylim(50., 115.)\n",
    "    pyplot.xlim(0, epochs_plotted)\n",
    "\n",
    "    axis = curves_figure.add_subplot(1, 2, 2)\n",
    "    epochs_plotted = plot_curve(axis, parameters, train_column = \"train_loss_history\", valid_column = \"valid_loss_history\")\n",
    "\n",
    "    pyplot.grid()\n",
    "    pyplot.legend()\n",
    "    pyplot.xlabel(\"epoch\")\n",
    "    pyplot.ylabel(\"loss\")\n",
    "    pyplot.ylim(0.0001, 10.)\n",
    "    pyplot.xlim(0, epochs_plotted)\n",
    "    pyplot.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Paths(object):\n",
    "    \"\"\"\n",
    "    Provides easy access to common paths we use for persisting \n",
    "    the data associated with model training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        \"\"\"\n",
    "        Initialises a new `Paths` instance and creates corresponding folders if needed.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params  : Parameters\n",
    "                  Structure (`namedtuple`) containing model parameters.\n",
    "        \"\"\"\n",
    "        self.model_name = self.get_model_name(params)\n",
    "        self.var_scope = self.get_variables_scope(params)\n",
    "        self.root_path = os.getcwd() + \"/models/\" + self.model_name + \"/\"\n",
    "        self.model_path = self.get_model_path()\n",
    "        self.train_history_path = self.get_train_history_path()\n",
    "        self.learning_curves_path = self.get_learning_curves_path()\n",
    "        if not os.path.exists(self.root_path):\n",
    "            os.makedirs(self.root_path)\n",
    "\n",
    "    def get_model_name(self, params):\n",
    "        \"\"\"\n",
    "        Generates a model name with some of the crucial model parameters encoded into the name.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params  : Parameters\n",
    "                  Structure (`namedtuple`) containing model parameters.\n",
    "                  \n",
    "        Returns\n",
    "        -------\n",
    "        Model name.\n",
    "        \"\"\"\n",
    "        # We will encode model settings in its name: architecture, optimisations applied, etc.\n",
    "        model_name = \"k{}d{}p{}_k{}d{}p{}_k{}d{}p{}_fc{}p{}\".format(\n",
    "            params.conv1_k, params.conv1_d, params.conv1_p, \n",
    "            params.conv2_k, params.conv2_d, params.conv2_p, \n",
    "            params.conv3_k, params.conv3_d, params.conv3_p, \n",
    "            params.fc4_size, params.fc4_p\n",
    "        )\n",
    "        model_name += \"_lrdec\" if params.learning_rate_decay else \"_no-lrdec\"\n",
    "        model_name += \"_l2\" if params.l2_reg_enabled else \"_no-l2\"\n",
    "        return model_name\n",
    "\n",
    "    def get_variables_scope(self, params):\n",
    "        \"\"\"\n",
    "        Generates a model variable scope with some of the crucial model parameters encoded.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params  : Parameters\n",
    "                  Structure (`namedtuple`) containing model parameters.\n",
    "                  \n",
    "        Returns\n",
    "        -------\n",
    "        Variables scope name.\n",
    "        \"\"\"\n",
    "        # We will encode model settings in its name: architecture, optimisations applied, etc.\n",
    "        var_scope = \"k{}d{}_k{}d{}_k{}d{}_fc{}_fc0\".format(\n",
    "            params.conv1_k, params.conv1_d,\n",
    "            params.conv2_k, params.conv2_d,\n",
    "            params.conv3_k, params.conv3_d, \n",
    "            params.fc4_size\n",
    "        )\n",
    "        return var_scope\n",
    "\n",
    "    def get_model_path(self):\n",
    "        \"\"\"\n",
    "        Generates path to the model file.\n",
    "   \n",
    "        Returns\n",
    "        -------\n",
    "        Model file path.\n",
    "        \"\"\"\n",
    "        return self.root_path + \"model.ckpt\"\n",
    "\n",
    "    def get_train_history_path(self):\n",
    "        \"\"\"\n",
    "        Generates path to the train history file.\n",
    "   \n",
    "        Returns\n",
    "        -------\n",
    "        Train history file path.\n",
    "        \"\"\"\n",
    "        return self.root_path + \"train_history\"\n",
    "    \n",
    "    def get_learning_curves_path(self):\n",
    "        \"\"\"\n",
    "        Generates path to the learning curves graph file.\n",
    "   \n",
    "        Returns\n",
    "        -------\n",
    "        Learning curves file path.\n",
    "        \"\"\"\n",
    "        return self.root_path + \"learning_curves.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(input, size):\n",
    "    \"\"\"\n",
    "    Performs a single fully connected layer pass, e.g. returns `input * weights + bias`.\n",
    "    \"\"\"\n",
    "    weights = tf.get_variable( 'weights', \n",
    "        shape = [input.get_shape()[1], size],\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "      )\n",
    "    biases = tf.get_variable( 'biases',\n",
    "        shape = [size],\n",
    "        initializer = tf.constant_initializer(0.0)\n",
    "      )\n",
    "    return tf.matmul(input, weights) + biases\n",
    "\n",
    "def fully_connected_relu(input, size):\n",
    "    return tf.nn.relu(fully_connected(input, size))\n",
    "\n",
    "def conv_relu(input, kernel_size, depth):\n",
    "    \"\"\"\n",
    "    Performs a single convolution layer pass.\n",
    "    \"\"\"\n",
    "    weights = tf.get_variable( 'weights', \n",
    "        shape = [kernel_size, kernel_size, input.get_shape()[3], depth],\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "      )\n",
    "    biases = tf.get_variable( 'biases',\n",
    "        shape = [depth],\n",
    "        initializer = tf.constant_initializer(0.0)\n",
    "      )\n",
    "    conv = tf.nn.conv2d(input, weights,\n",
    "        strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def pool(input, size):\n",
    "    \"\"\"\n",
    "    Performs a max pooling layer pass.\n",
    "    \"\"\"\n",
    "    return tf.nn.max_pool(\n",
    "        input, \n",
    "        ksize = [1, size, size, 1], \n",
    "        strides = [1, size, size, 1], \n",
    "        padding = 'SAME'\n",
    "    )\n",
    "\n",
    "def model_pass(input, params, is_training):\n",
    "    # Convolutions\n",
    "\n",
    "    with tf.variable_scope('conv1'):\n",
    "        conv1 = conv_relu(input, kernel_size = params.conv1_k, depth = params.conv1_d) \n",
    "    with tf.variable_scope('pool1'): \n",
    "        pool1 = pool(conv1, size = 2)\n",
    "        pool1 = tf.cond(is_training, lambda: tf.nn.dropout(pool1, keep_prob = params.conv1_p), lambda: pool1)\n",
    "    with tf.variable_scope('conv2'):\n",
    "        conv2 = conv_relu(pool1, kernel_size = params.conv2_k, depth = params.conv2_d)\n",
    "    with tf.variable_scope('pool2'):\n",
    "        pool2 = pool(conv2, size = 2)\n",
    "        pool2 = tf.cond(is_training, lambda: tf.nn.dropout(pool2, keep_prob = params.conv2_p), lambda: pool2)\n",
    "    with tf.variable_scope('conv3'):\n",
    "        conv3 = conv_relu(pool2, kernel_size = params.conv3_k, depth = params.conv3_d)\n",
    "    with tf.variable_scope('pool3'):\n",
    "        pool3 = pool(conv3, size = 2)\n",
    "        pool3 = tf.cond(is_training, lambda: tf.nn.dropout(pool3, keep_prob = params.conv3_p), lambda: pool3)\n",
    "    \n",
    "    # Fully connected\n",
    "    \n",
    "    # 1st stage output\n",
    "    pool1 = pool(pool1, size = 4)\n",
    "    shape = pool1.get_shape().as_list()\n",
    "    pool1 = tf.reshape(pool1, [-1, shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    # 2nd stage output\n",
    "    pool2 = pool(pool2, size = 2)\n",
    "    shape = pool2.get_shape().as_list()\n",
    "    pool2 = tf.reshape(pool2, [-1, shape[1] * shape[2] * shape[3]])    \n",
    "    \n",
    "    # 3rd stage output\n",
    "    shape = pool3.get_shape().as_list()\n",
    "    pool3 = tf.reshape(pool3, [-1, shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    flattened = tf.concat([pool1, pool2, pool3], 1)\n",
    "    \n",
    "    with tf.variable_scope('fc4'):\n",
    "        fc4 = fully_connected_relu(flattened, size = params.fc4_size)\n",
    "        fc4 = tf.cond(is_training, lambda: tf.nn.dropout(fc4, keep_prob = params.fc4_p), lambda: fc4)\n",
    "    with tf.variable_scope('out'):\n",
    "        logits = fully_connected(fc4, size = params.num_classes)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(params, X_train, y_train, X_valid, y_valid, X_test, y_test):\n",
    "    \n",
    "    # Initialisation routines: generate variable scope, create logger, note start time.\n",
    "    paths = Paths(params)\n",
    "    start = time.time()\n",
    "    model_variable_scope = paths.var_scope\n",
    "\n",
    "    \n",
    "    # Build the graph\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Input data. For the training data, we use a placeholder that will be fed at run time with a training minibatch.\n",
    "        tf_x_batch = tf.placeholder(tf.float32, shape = (None, params.image_size[0], params.image_size[1], 1))\n",
    "        tf_y_batch = tf.placeholder(tf.float32, shape = (None, params.num_classes))\n",
    "        is_training = tf.placeholder(tf.bool)\n",
    "        current_epoch = tf.Variable(0, trainable=False)  # count the number of epochs\n",
    "\n",
    "        # Model parameters.\n",
    "        if params.learning_rate_decay:\n",
    "            learning_rate = tf.train.exponential_decay(params.learning_rate, current_epoch, decay_steps = params.max_epochs, decay_rate = 0.01)\n",
    "        else:\n",
    "            learning_rate = params.learning_rate\n",
    "            \n",
    "        # Training computation.\n",
    "        with tf.variable_scope(model_variable_scope):\n",
    "            logits = model_pass(tf_x_batch, params, is_training)\n",
    "            if params.l2_reg_enabled:\n",
    "                with tf.variable_scope('fc4', reuse = True):\n",
    "                    l2_loss = tf.nn.l2_loss(tf.get_variable('weights'))\n",
    "            else:\n",
    "                l2_loss = 0\n",
    "\n",
    "        predictions = tf.nn.softmax(logits)\n",
    "        softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=tf_y_batch, logits=logits)\n",
    "        loss = tf.reduce_mean(softmax_cross_entropy) + params.l2_lambda * l2_loss  \n",
    "\n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate = learning_rate\n",
    "        ).minimize(loss)\n",
    "\n",
    "    with tf.Session(graph = graph) as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        # A routine for evaluating current model parameters\n",
    "        def get_accuracy_and_loss_in_batches(X, y, is_test=False):\n",
    "            p = []\n",
    "            sce = []\n",
    "            batch_iterator = BatchIterator(batch_size = 128)\n",
    "            for x_batch, y_batch in batch_iterator(X, y):\n",
    "                [p_batch, sce_batch] = session.run([predictions, softmax_cross_entropy], feed_dict = {\n",
    "                        tf_x_batch : x_batch, \n",
    "                        tf_y_batch : y_batch,\n",
    "                        is_training : False\n",
    "                    }\n",
    "                )\n",
    "                p.extend(p_batch)\n",
    "                sce.extend(sce_batch)                \n",
    "            p = np.array(p)\n",
    "            sce = np.array(sce)\n",
    "            accuracy = 100.0 * np.sum(np.argmax(p, 1) == np.argmax(y, 1)) / p.shape[0]\n",
    "            loss = np.mean(sce)\n",
    "            if is_test:\n",
    "                print (\"test accuracy \", accuracy, \" test loss \", loss)\n",
    "            return (accuracy, loss)\n",
    "        \n",
    "        # If we chose to keep training previously trained model, restore session.\n",
    "        if params.resume_training: \n",
    "            try:\n",
    "                tf.train.Saver().restore(session, paths.model_path)\n",
    "            except Exception as e:\n",
    "                print(\"Failed restoring previously trained model: file does not exist.\")\n",
    "                pass\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "#         early_stopping = EarlyStopping(tf.train.Saver(), session, patience = params.early_stopping_patience, minimize = True)\n",
    "        train_loss_history = np.empty([0], dtype = np.float32)\n",
    "        train_accuracy_history = np.empty([0], dtype = np.float32)\n",
    "        valid_loss_history = np.empty([0], dtype = np.float32)\n",
    "        valid_accuracy_history = np.empty([0], dtype = np.float32)\n",
    "        if params.max_epochs > 0:\n",
    "            print(\"================= TRAINING ==================\")\n",
    "        else:\n",
    "            print(\"================== TESTING ==================\")\n",
    "        \n",
    "        for epoch in range(params.max_epochs):\n",
    "            current_epoch = epoch\n",
    "            print (\"current epoch \", current_epoch)\n",
    "            # Train on whole randomised dataset in batches\n",
    "            batch_iterator = BatchIterator(batch_size = params.batch_size, shuffle = True)\n",
    "            batch_cnt=0\n",
    "            for x_batch, y_batch in batch_iterator(X_train, y_train):\n",
    "                batch_cnt+=1\n",
    "#                 print (\"batch \", batch_cnt)\n",
    "                session.run([optimizer], feed_dict = {\n",
    "                        tf_x_batch : x_batch, \n",
    "                        tf_y_batch : y_batch,\n",
    "                        is_training : True\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # If another significant epoch ended, we log our losses.\n",
    "            if (epoch % params.log_epoch == 0):\n",
    "                # Get validation data predictions and log validation loss:\n",
    "                valid_accuracy, valid_loss = get_accuracy_and_loss_in_batches(X_valid, y_valid)\n",
    "\n",
    "                # Get training data predictions and log training loss:\n",
    "                train_accuracy, train_loss = get_accuracy_and_loss_in_batches(X_train, y_train)\n",
    "                print (\"train accuracy \", train_accuracy, \" train loss \", train_loss)\n",
    "                print (\"valid accuracy \", valid_accuracy, \" valid loss \", valid_loss)\n",
    "            else:\n",
    "                valid_loss = 0.\n",
    "                valid_accuracy = 0.\n",
    "                train_loss = 0.\n",
    "                train_accuracy = 0.\n",
    "                \n",
    "            valid_loss_history = np.append(valid_loss_history, [valid_loss])\n",
    "            valid_accuracy_history = np.append(valid_accuracy_history, [valid_accuracy])\n",
    "            train_loss_history = np.append(train_loss_history, [train_loss])\n",
    "            train_accuracy_history = np.append(train_accuracy_history, [train_accuracy])\n",
    "            \n",
    "\n",
    "        # Evaluate on test dataset.\n",
    "        test_accuracy, test_loss = get_accuracy_and_loss_in_batches(X_test, y_test)\n",
    "#         valid_accuracy, valid_loss = get_accuracy_and_loss_in_batches(X_valid, y_valid)\n",
    "        # Save model weights for future use.\n",
    "        saved_model_path = saver.save(session, paths.model_path)\n",
    "\n",
    "        np.savez(paths.train_history_path, train_loss_history = train_loss_history, train_accuracy_history = train_accuracy_history, valid_loss_history = valid_loss_history, valid_accuracy_history = valid_accuracy_history)\n",
    "    \n",
    "        \n",
    "#         plot_learning_curves(params)\n",
    "#         pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = Parameters(\n",
    "    # Data parameters\n",
    "    num_classes = NUM_CLASS,\n",
    "    image_size = (32, 32),\n",
    "    # Training parameters\n",
    "    batch_size = 256,\n",
    "    max_epochs = 100,\n",
    "    log_epoch = 1,\n",
    "    print_epoch = 1,\n",
    "    # Optimisations\n",
    "    learning_rate_decay = False,\n",
    "    learning_rate = 0.0001,\n",
    "    l2_reg_enabled = True,\n",
    "    l2_lambda = 0.0001,\n",
    "    early_stopping_enabled = True,\n",
    "    early_stopping_patience = 100,\n",
    "    resume_training = True,\n",
    "    # Layers architecture\n",
    "    conv1_k = 5, conv1_d = 32, conv1_p = 0.9,\n",
    "    conv2_k = 5, conv2_d = 64, conv2_p = 0.8,\n",
    "    conv3_k = 5, conv3_d = 128, conv3_p = 0.7,\n",
    "    fc4_size = 1024, fc4_p = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(parameters,X_train, y_train, X_valid, y_valid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_predictions(params, X, k = 5):\n",
    "    \n",
    "    # Initialisation routines: generate variable scope, create logger, note start time.\n",
    "    paths = Paths(params)\n",
    "    \n",
    "    # Build the graph\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Input data. For the training data, we use a placeholder that will be fed at run time with a training minibatch.\n",
    "        tf_x = tf.placeholder(tf.float32, shape = (None, params.image_size[0], params.image_size[1], 1))\n",
    "        is_training = tf.constant(False)\n",
    "        with tf.variable_scope(paths.var_scope):\n",
    "            predictions = tf.nn.softmax(model_pass(tf_x, params, is_training))\n",
    "            top_k_predictions = tf.nn.top_k(predictions, k)\n",
    "\n",
    "    with tf.Session(graph = graph) as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        tf.train.Saver().restore(session, paths.model_path)\n",
    "        [p] = session.run([top_k_predictions], feed_dict = {\n",
    "                tf_x : X\n",
    "            }\n",
    "        )\n",
    "        return np.array(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/tuyenlv17/data/programming/projects/digital-race-2/GSS-CNN-classification/models/k5d32p0.9_k5d64p0.8_k5d128p0.7_fc1024p0.5_no-lrdec_l2/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[9.99563396e-01, 4.35954105e-04, 2.87375258e-07],\n",
       "        [1.00000000e+00, 2.41136160e-08, 2.89371324e-14],\n",
       "        [9.99994159e-01, 1.67696180e-06, 1.23203756e-06],\n",
       "        ...,\n",
       "        [9.99998212e-01, 1.00875945e-06, 4.92400545e-07],\n",
       "        [9.99972939e-01, 2.67494452e-05, 2.91903945e-07],\n",
       "        [9.99984264e-01, 1.43735579e-05, 8.14097007e-07]],\n",
       "\n",
       "       [[0.00000000e+00, 5.00000000e+00, 7.00000000e+00],\n",
       "        [9.00000000e+00, 5.00000000e+00, 3.00000000e+00],\n",
       "        [2.00000000e+00, 0.00000000e+00, 5.00000000e+00],\n",
       "        ...,\n",
       "        [6.00000000e+00, 3.00000000e+00, 9.00000000e+00],\n",
       "        [3.00000000e+00, 2.00000000e+00, 6.00000000e+00],\n",
       "        [5.00000000e+00, 1.00000000e+00, 9.00000000e+00]]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_predictions(parameters, X_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tuyenlv17/data/programming/projects/digital-race-2/GSS-CNN-classification/models/k5d32p0.9_k5d64p0.8_k5d128p0.7_fc1024p0.5_no-lrdec_l2/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/tuyenlv17/data/programming/projects/digital-race-2/GSS-CNN-classification/models/k5d32p0.9_k5d64p0.8_k5d128p0.7_fc1024p0.5_no-lrdec_l2/model.ckpt\n",
      "7 1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python-virtualenv/ml-python3/lib/python3.5/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100.png\n",
      "0 101.png\n",
      "0 102.png\n",
      "0 103.png\n",
      "0 104.png\n",
      "0 105.png\n",
      "0 106.png\n",
      "0 107.png\n",
      "0 108.png\n",
      "0 109.png\n",
      "0 110.png\n",
      "0 111.png\n",
      "0 112.png\n",
      "0 113.png\n",
      "0 114.png\n",
      "7 29.png\n",
      "4 470.png\n",
      "7 477.png\n",
      "7 487.png\n",
      "0 95.png\n",
      "0 96.png\n",
      "0 97.png\n",
      "0 98.png\n",
      "0 99.png\n",
      "0 13.png\n",
      "7 135.png\n",
      "7 136.png\n",
      "7 25.png\n",
      "7 28.png\n",
      "2 317.png\n",
      "0 393.png\n",
      "1 583.png\n",
      "7 137.png\n",
      "7 138.png\n",
      "7 139.png\n",
      "7 140.png\n",
      "7 141.png\n",
      "7 142.png\n",
      "7 143.png\n",
      "7 144.png\n",
      "7 145.png\n",
      "7 146.png\n",
      "7 147.png\n",
      "7 148.png\n",
      "7 149.png\n",
      "7 15.png\n",
      "7 150.png\n",
      "7 151.png\n",
      "7 152.png\n",
      "7 153.png\n",
      "7 154.png\n",
      "7 16.png\n",
      "7 17.png\n",
      "7 18.png\n",
      "7 19.png\n",
      "7 20.png\n",
      "7 21.png\n",
      "7 22.png\n",
      "7 23.png\n",
      "7 24.png\n",
      "7 26.png\n",
      "7 27.png\n",
      "7 30.png\n",
      "7 31.png\n",
      "7 314.png\n",
      "0 316.png\n",
      "0 318.png\n",
      "0 319.png\n",
      "7 32.png\n",
      "0 321.png\n",
      "0 322.png\n",
      "0 323.png\n",
      "0 324.png\n",
      "0 325.png\n",
      "3 326.png\n",
      "0 327.png\n",
      "0 328.png\n",
      "0 329.png\n",
      "7 33.png\n",
      "8 330.png\n",
      "8 331.png\n",
      "3 335.png\n",
      "7 34.png\n",
      "7 342.png\n",
      "1 344.png\n",
      "3 348.png\n",
      "3 363.png\n",
      "3 364.png\n",
      "3 365.png\n",
      "3 366.png\n",
      "7 369.png\n",
      "7 371.png\n",
      "7 372.png\n",
      "3 373.png\n",
      "3 375.png\n",
      "3 376.png\n",
      "3 379.png\n",
      "3 381.png\n",
      "3 382.png\n",
      "0 383.png\n",
      "7 385.png\n",
      "2 386.png\n",
      "7 387.png\n",
      "0 391.png\n",
      "7 395.png\n",
      "7 396.png\n",
      "7 400.png\n",
      "0 401.png\n",
      "0 402.png\n",
      "2 403.png\n",
      "0 404.png\n",
      "0 405.png\n",
      "0 406.png\n",
      "0 407.png\n",
      "7 408.png\n",
      "8 409.png\n",
      "2 412.png\n",
      "2 423.png\n",
      "7 429.png\n",
      "2 431.png\n",
      "7 432.png\n",
      "7 437.png\n",
      "7 438.png\n",
      "7 442.png\n",
      "7 443.png\n",
      "7 444.png\n",
      "2 446.png\n",
      "7 447.png\n",
      "7 448.png\n",
      "7 449.png\n",
      "7 450.png\n",
      "7 451.png\n",
      "1 452.png\n",
      "7 453.png\n",
      "7 454.png\n",
      "7 455.png\n",
      "1 456.png\n",
      "7 457.png\n",
      "7 458.png\n",
      "7 459.png\n",
      "7 460.png\n",
      "7 461.png\n",
      "4 463.png\n",
      "4 464.png\n",
      "7 466.png\n",
      "7 498.png\n",
      "1 509.png\n",
      "7 520.png\n",
      "7 530.png\n",
      "7 541.png\n",
      "7 551.png\n",
      "7 572.png\n",
      "3 590.png\n",
      "7 595.png\n",
      "7 598.png\n",
      "7 603.png\n",
      "7 606.png\n",
      "7 611.png\n",
      "7 612.png\n",
      "7 613.png\n",
      "7 614.png\n",
      "4 627.png\n"
     ]
    }
   ],
   "source": [
    "from skimage import io\n",
    "from skimage import transform as ski_transform\n",
    "from skimage import color as ski_color\n",
    "def add_path_prefix(c):\n",
    "    return \"0\" + str(c)\n",
    "base_target_path = './signs/target/'\n",
    "# def image_flip():\n",
    "#     for cur_class in range(num_class):\n",
    "paths = Paths(parameters)\n",
    "graph = tf.Graph()\n",
    "print (paths.model_path)\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed at run time with a training minibatch.\n",
    "    tf_x = tf.placeholder(tf.float32, shape = (None, parameters.image_size[0], parameters.image_size[1], 1))\n",
    "    is_training = tf.constant(False)\n",
    "    with tf.variable_scope(paths.var_scope):\n",
    "        predictions = tf.nn.softmax(model_pass(tf_x, parameters, is_training))\n",
    "        top_k_predictions = tf.nn.top_k(predictions, 5)\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    tf.train.Saver().restore(session, paths.model_path)   \n",
    "    cur_class = 6\n",
    "    list_files=os.listdir(base_target_path + \"/\" + add_path_prefix(cur_class))\n",
    "    class_base_path = base_target_path + \"/\" + add_path_prefix(cur_class) + \"/\"\n",
    "    img_iter = 0\n",
    "    for index, cur_file in enumerate(list_files):\n",
    "        roi = io.imread(class_base_path + \"/\" + cur_file)            \n",
    "        roi = ski_transform.resize(roi, (32, 32))\n",
    "        roi = ski_color.rgb2gray(roi)\n",
    "        roi = roi.reshape(roi.shape + (1,))        \n",
    "        tf_X = np.array([roi])\n",
    "        [prediction] = session.run([top_k_predictions], feed_dict = {\n",
    "                tf_x : tf_X\n",
    "            }\n",
    "        )\n",
    "        class_int = prediction[1][0][0]\n",
    "        if class_int != cur_class:\n",
    "            print (class_int, cur_file)\n",
    "#         print (prediction[1][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
