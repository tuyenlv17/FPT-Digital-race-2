{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import os\n",
    "from skimage import io as ski_io\n",
    "from skimage.viewer import ImageViewer\n",
    "import time\n",
    "from nolearn.lasagne import BatchIterator\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_path_prefix(c):\n",
    "    return \"0\" + str(c)\n",
    "def one_hot(labels, n_class = NUM_CLASS):\n",
    "    total_record = len(labels)\n",
    "    y = np.zeros((total_record, n_class))\n",
    "    y[np.arange(total_record), labels] = 1\n",
    "    return y\n",
    "\n",
    "def load_data():\n",
    "    X = []\n",
    "    y = []\n",
    "    path = './signs-gray-32x32'\n",
    "    for i in range(10):\n",
    "        class_path=path + \"/\" + add_path_prefix(i)\n",
    "        list_files=os.listdir(class_path)\n",
    "        for cur_file in list_files:\n",
    "            file_path = class_path + \"/\" + cur_file\n",
    "            img = ski_io.imread(file_path, as_grey=True)\n",
    "            img = (img / 255.).astype(np.float32)\n",
    "#             print (file_path)\n",
    "#             print (img)\n",
    "#             return \n",
    "            img = img.reshape(img.shape + (1,)) \n",
    "            X.append(img)\n",
    "            y.append(i)\n",
    "    y = one_hot(np.array(y))\n",
    "    return np.array(X, dtype=np.float32), y\n",
    "X_train, y_train = load_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2, random_state=12132)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.1, random_state=54651)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Parameters = namedtuple('Parameters', [\n",
    "        # Data parameters\n",
    "        'num_classes', 'image_size', \n",
    "        # Training parameters\n",
    "        'batch_size', 'max_epochs', 'log_epoch', 'print_epoch',\n",
    "        # Optimisations\n",
    "        'learning_rate_decay', 'learning_rate',\n",
    "        'l2_reg_enabled', 'l2_lambda', \n",
    "        'early_stopping_enabled', 'early_stopping_patience', \n",
    "        'resume_training', \n",
    "        # Layers architecture\n",
    "        'conv1_k', 'conv1_d', 'conv1_p', \n",
    "        'conv2_k', 'conv2_d', 'conv2_p', \n",
    "        'conv3_k', 'conv3_d', 'conv3_p', \n",
    "        'fc4_size', 'fc4_p'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(axis, params, train_column, valid_column, linewidth = 2, train_linestyle = \"b-\", valid_linestyle = \"g-\"):\n",
    "    \"\"\"\n",
    "    Plots a pair of validation and training curves on a single plot.\n",
    "    \"\"\"\n",
    "    model_history = np.load(Paths(params).train_history_path + \".npz\")\n",
    "    train_values = model_history[train_column]\n",
    "    valid_values = model_history[valid_column]\n",
    "    epochs = train_values.shape[0]\n",
    "    x_axis = np.arange(epochs)\n",
    "    axis.plot(x_axis[train_values > 0], train_values[train_values > 0], train_linestyle, linewidth=linewidth, label=\"train\")\n",
    "    axis.plot(x_axis[valid_values > 0], valid_values[valid_values > 0], valid_linestyle, linewidth=linewidth, label=\"valid\")\n",
    "    return epochs\n",
    "\n",
    "# Plots history of learning curves for a specific model.\n",
    "def plot_learning_curves(params):\n",
    "    \"\"\"\n",
    "    Plots learning curves (loss and accuracy on both training and validation sets) for a model identified by a parameters struct.\n",
    "    \"\"\"\n",
    "    curves_figure = pyplot.figure(figsize = (10, 4))\n",
    "    axis = curves_figure.add_subplot(1, 2, 1)\n",
    "    epochs_plotted = plot_curve(axis, parameters, train_column = \"train_accuracy_history\", valid_column = \"valid_accuracy_history\")\n",
    "\n",
    "    pyplot.grid()\n",
    "    pyplot.legend()\n",
    "    pyplot.xlabel(\"epoch\")\n",
    "    pyplot.ylabel(\"accuracy\")\n",
    "    pyplot.ylim(50., 115.)\n",
    "    pyplot.xlim(0, epochs_plotted)\n",
    "\n",
    "    axis = curves_figure.add_subplot(1, 2, 2)\n",
    "    epochs_plotted = plot_curve(axis, parameters, train_column = \"train_loss_history\", valid_column = \"valid_loss_history\")\n",
    "\n",
    "    pyplot.grid()\n",
    "    pyplot.legend()\n",
    "    pyplot.xlabel(\"epoch\")\n",
    "    pyplot.ylabel(\"loss\")\n",
    "    pyplot.ylim(0.0001, 10.)\n",
    "    pyplot.xlim(0, epochs_plotted)\n",
    "    pyplot.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Paths(object):\n",
    "    \"\"\"\n",
    "    Provides easy access to common paths we use for persisting \n",
    "    the data associated with model training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        \"\"\"\n",
    "        Initialises a new `Paths` instance and creates corresponding folders if needed.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params  : Parameters\n",
    "                  Structure (`namedtuple`) containing model parameters.\n",
    "        \"\"\"\n",
    "        self.model_name = self.get_model_name(params)\n",
    "        self.var_scope = self.get_variables_scope(params)\n",
    "        self.root_path = os.getcwd() + \"/models/\" + self.model_name + \"/\"\n",
    "        self.model_path = self.get_model_path()\n",
    "        self.train_history_path = self.get_train_history_path()\n",
    "        self.learning_curves_path = self.get_learning_curves_path()\n",
    "        os.makedirs(self.root_path, exist_ok = True)\n",
    "\n",
    "    def get_model_name(self, params):\n",
    "        \"\"\"\n",
    "        Generates a model name with some of the crucial model parameters encoded into the name.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params  : Parameters\n",
    "                  Structure (`namedtuple`) containing model parameters.\n",
    "                  \n",
    "        Returns\n",
    "        -------\n",
    "        Model name.\n",
    "        \"\"\"\n",
    "        # We will encode model settings in its name: architecture, optimisations applied, etc.\n",
    "        model_name = \"k{}d{}p{}_k{}d{}p{}_k{}d{}p{}_fc{}p{}\".format(\n",
    "            params.conv1_k, params.conv1_d, params.conv1_p, \n",
    "            params.conv2_k, params.conv2_d, params.conv2_p, \n",
    "            params.conv3_k, params.conv3_d, params.conv3_p, \n",
    "            params.fc4_size, params.fc4_p\n",
    "        )\n",
    "        model_name += \"_lrdec\" if params.learning_rate_decay else \"_no-lrdec\"\n",
    "        model_name += \"_l2\" if params.l2_reg_enabled else \"_no-l2\"\n",
    "        return model_name\n",
    "\n",
    "    def get_variables_scope(self, params):\n",
    "        \"\"\"\n",
    "        Generates a model variable scope with some of the crucial model parameters encoded.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params  : Parameters\n",
    "                  Structure (`namedtuple`) containing model parameters.\n",
    "                  \n",
    "        Returns\n",
    "        -------\n",
    "        Variables scope name.\n",
    "        \"\"\"\n",
    "        # We will encode model settings in its name: architecture, optimisations applied, etc.\n",
    "        var_scope = \"k{}d{}_k{}d{}_k{}d{}_fc{}_fc0\".format(\n",
    "            params.conv1_k, params.conv1_d,\n",
    "            params.conv2_k, params.conv2_d,\n",
    "            params.conv3_k, params.conv3_d, \n",
    "            params.fc4_size\n",
    "        )\n",
    "        return var_scope\n",
    "\n",
    "    def get_model_path(self):\n",
    "        \"\"\"\n",
    "        Generates path to the model file.\n",
    "   \n",
    "        Returns\n",
    "        -------\n",
    "        Model file path.\n",
    "        \"\"\"\n",
    "        return self.root_path + \"model.ckpt\"\n",
    "\n",
    "    def get_train_history_path(self):\n",
    "        \"\"\"\n",
    "        Generates path to the train history file.\n",
    "   \n",
    "        Returns\n",
    "        -------\n",
    "        Train history file path.\n",
    "        \"\"\"\n",
    "        return self.root_path + \"train_history\"\n",
    "    \n",
    "    def get_learning_curves_path(self):\n",
    "        \"\"\"\n",
    "        Generates path to the learning curves graph file.\n",
    "   \n",
    "        Returns\n",
    "        -------\n",
    "        Learning curves file path.\n",
    "        \"\"\"\n",
    "        return self.root_path + \"learning_curves.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(input, size):\n",
    "    \"\"\"\n",
    "    Performs a single fully connected layer pass, e.g. returns `input * weights + bias`.\n",
    "    \"\"\"\n",
    "    weights = tf.get_variable( 'weights', \n",
    "        shape = [input.get_shape()[1], size],\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "      )\n",
    "    biases = tf.get_variable( 'biases',\n",
    "        shape = [size],\n",
    "        initializer = tf.constant_initializer(0.0)\n",
    "      )\n",
    "    return tf.matmul(input, weights) + biases\n",
    "\n",
    "def fully_connected_relu(input, size):\n",
    "    return tf.nn.relu(fully_connected(input, size))\n",
    "\n",
    "def conv_relu(input, kernel_size, depth):\n",
    "    \"\"\"\n",
    "    Performs a single convolution layer pass.\n",
    "    \"\"\"\n",
    "    weights = tf.get_variable( 'weights', \n",
    "        shape = [kernel_size, kernel_size, input.get_shape()[3], depth],\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "      )\n",
    "    biases = tf.get_variable( 'biases',\n",
    "        shape = [depth],\n",
    "        initializer = tf.constant_initializer(0.0)\n",
    "      )\n",
    "    conv = tf.nn.conv2d(input, weights,\n",
    "        strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def pool(input, size):\n",
    "    \"\"\"\n",
    "    Performs a max pooling layer pass.\n",
    "    \"\"\"\n",
    "    return tf.nn.max_pool(\n",
    "        input, \n",
    "        ksize = [1, size, size, 1], \n",
    "        strides = [1, size, size, 1], \n",
    "        padding = 'SAME'\n",
    "    )\n",
    "\n",
    "def model_pass(input, params, is_training):\n",
    "    # Convolutions\n",
    "\n",
    "    with tf.variable_scope('conv1'):\n",
    "        conv1 = conv_relu(input, kernel_size = params.conv1_k, depth = params.conv1_d) \n",
    "    with tf.variable_scope('pool1'): \n",
    "        pool1 = pool(conv1, size = 2)\n",
    "        pool1 = tf.cond(is_training, lambda: tf.nn.dropout(pool1, keep_prob = params.conv1_p), lambda: pool1)\n",
    "    with tf.variable_scope('conv2'):\n",
    "        conv2 = conv_relu(pool1, kernel_size = params.conv2_k, depth = params.conv2_d)\n",
    "    with tf.variable_scope('pool2'):\n",
    "        pool2 = pool(conv2, size = 2)\n",
    "        pool2 = tf.cond(is_training, lambda: tf.nn.dropout(pool2, keep_prob = params.conv2_p), lambda: pool2)\n",
    "    with tf.variable_scope('conv3'):\n",
    "        conv3 = conv_relu(pool2, kernel_size = params.conv3_k, depth = params.conv3_d)\n",
    "    with tf.variable_scope('pool3'):\n",
    "        pool3 = pool(conv3, size = 2)\n",
    "        pool3 = tf.cond(is_training, lambda: tf.nn.dropout(pool3, keep_prob = params.conv3_p), lambda: pool3)\n",
    "    \n",
    "    # Fully connected\n",
    "    \n",
    "    # 1st stage output\n",
    "    pool1 = pool(pool1, size = 4)\n",
    "    shape = pool1.get_shape().as_list()\n",
    "    pool1 = tf.reshape(pool1, [-1, shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    # 2nd stage output\n",
    "    pool2 = pool(pool2, size = 2)\n",
    "    shape = pool2.get_shape().as_list()\n",
    "    pool2 = tf.reshape(pool2, [-1, shape[1] * shape[2] * shape[3]])    \n",
    "    \n",
    "    # 3rd stage output\n",
    "    shape = pool3.get_shape().as_list()\n",
    "    pool3 = tf.reshape(pool3, [-1, shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    flattened = tf.concat([pool1, pool2, pool3], 1)\n",
    "    \n",
    "    with tf.variable_scope('fc4'):\n",
    "        fc4 = fully_connected_relu(flattened, size = params.fc4_size)\n",
    "        fc4 = tf.cond(is_training, lambda: tf.nn.dropout(fc4, keep_prob = params.fc4_p), lambda: fc4)\n",
    "    with tf.variable_scope('out'):\n",
    "        logits = fully_connected(fc4, size = params.num_classes)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(params, X_train, y_train, X_valid, y_valid, X_test, y_test):\n",
    "    \n",
    "    # Initialisation routines: generate variable scope, create logger, note start time.\n",
    "    paths = Paths(params)\n",
    "    start = time.time()\n",
    "    model_variable_scope = paths.var_scope\n",
    "\n",
    "    \n",
    "    # Build the graph\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Input data. For the training data, we use a placeholder that will be fed at run time with a training minibatch.\n",
    "        tf_x_batch = tf.placeholder(tf.float32, shape = (None, params.image_size[0], params.image_size[1], 1))\n",
    "        tf_y_batch = tf.placeholder(tf.float32, shape = (None, params.num_classes))\n",
    "        is_training = tf.placeholder(tf.bool)\n",
    "        current_epoch = tf.Variable(0, trainable=False)  # count the number of epochs\n",
    "\n",
    "        # Model parameters.\n",
    "        if params.learning_rate_decay:\n",
    "            learning_rate = tf.train.exponential_decay(params.learning_rate, current_epoch, decay_steps = params.max_epochs, decay_rate = 0.01)\n",
    "        else:\n",
    "            learning_rate = params.learning_rate\n",
    "            \n",
    "        # Training computation.\n",
    "        with tf.variable_scope(model_variable_scope):\n",
    "            logits = model_pass(tf_x_batch, params, is_training)\n",
    "            if params.l2_reg_enabled:\n",
    "                with tf.variable_scope('fc4', reuse = True):\n",
    "                    l2_loss = tf.nn.l2_loss(tf.get_variable('weights'))\n",
    "            else:\n",
    "                l2_loss = 0\n",
    "\n",
    "        predictions = tf.nn.softmax(logits)\n",
    "        softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=tf_y_batch, logits=logits)\n",
    "        loss = tf.reduce_mean(softmax_cross_entropy) + params.l2_lambda * l2_loss  \n",
    "\n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate = learning_rate\n",
    "        ).minimize(loss)\n",
    "\n",
    "    with tf.Session(graph = graph) as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        # A routine for evaluating current model parameters\n",
    "        def get_accuracy_and_loss_in_batches(X, y, is_test=False):\n",
    "            p = []\n",
    "            sce = []\n",
    "            batch_iterator = BatchIterator(batch_size = 128)\n",
    "            for x_batch, y_batch in batch_iterator(X, y):\n",
    "                [p_batch, sce_batch] = session.run([predictions, softmax_cross_entropy], feed_dict = {\n",
    "                        tf_x_batch : x_batch, \n",
    "                        tf_y_batch : y_batch,\n",
    "                        is_training : False\n",
    "                    }\n",
    "                )\n",
    "                p.extend(p_batch)\n",
    "                sce.extend(sce_batch)                \n",
    "            p = np.array(p)\n",
    "            sce = np.array(sce)\n",
    "            accuracy = 100.0 * np.sum(np.argmax(p, 1) == np.argmax(y, 1)) / p.shape[0]\n",
    "            loss = np.mean(sce)\n",
    "            if is_test:\n",
    "                print (\"test accuracy \", accuracy, \" test loss \", loss)\n",
    "            return (accuracy, loss)\n",
    "        \n",
    "        # If we chose to keep training previously trained model, restore session.\n",
    "        if params.resume_training: \n",
    "            try:\n",
    "                tf.train.Saver().restore(session, paths.model_path)\n",
    "            except Exception as e:\n",
    "                print(\"Failed restoring previously trained model: file does not exist.\")\n",
    "                pass\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "#         early_stopping = EarlyStopping(tf.train.Saver(), session, patience = params.early_stopping_patience, minimize = True)\n",
    "        train_loss_history = np.empty([0], dtype = np.float32)\n",
    "        train_accuracy_history = np.empty([0], dtype = np.float32)\n",
    "        valid_loss_history = np.empty([0], dtype = np.float32)\n",
    "        valid_accuracy_history = np.empty([0], dtype = np.float32)\n",
    "        if params.max_epochs > 0:\n",
    "            print(\"================= TRAINING ==================\")\n",
    "        else:\n",
    "            print(\"================== TESTING ==================\")\n",
    "        \n",
    "        for epoch in range(params.max_epochs):\n",
    "            current_epoch = epoch\n",
    "            print (\"current epoch \", current_epoch)\n",
    "            # Train on whole randomised dataset in batches\n",
    "            batch_iterator = BatchIterator(batch_size = params.batch_size, shuffle = True)\n",
    "            batch_cnt=0\n",
    "            for x_batch, y_batch in batch_iterator(X_train, y_train):\n",
    "                batch_cnt+=1\n",
    "                print (\"batch \", batch_cnt)\n",
    "                session.run([optimizer], feed_dict = {\n",
    "                        tf_x_batch : x_batch, \n",
    "                        tf_y_batch : y_batch,\n",
    "                        is_training : True\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # If another significant epoch ended, we log our losses.\n",
    "            if (epoch % params.log_epoch == 0):\n",
    "                # Get validation data predictions and log validation loss:\n",
    "                valid_accuracy, valid_loss = get_accuracy_and_loss_in_batches(X_valid, y_valid)\n",
    "\n",
    "                # Get training data predictions and log training loss:\n",
    "                train_accuracy, train_loss = get_accuracy_and_loss_in_batches(X_train, y_train)\n",
    "                print (\"train accuracy \", train_accuracy, \" train loss \", train_loss)\n",
    "                print (\"valid accuracy \", valid_accuracy, \" valid loss \", valid_loss)\n",
    "            else:\n",
    "                valid_loss = 0.\n",
    "                valid_accuracy = 0.\n",
    "                train_loss = 0.\n",
    "                train_accuracy = 0.\n",
    "                \n",
    "            valid_loss_history = np.append(valid_loss_history, [valid_loss])\n",
    "            valid_accuracy_history = np.append(valid_accuracy_history, [valid_accuracy])\n",
    "            train_loss_history = np.append(train_loss_history, [train_loss])\n",
    "            train_accuracy_history = np.append(train_accuracy_history, [train_accuracy])\n",
    "            \n",
    "\n",
    "        # Evaluate on test dataset.\n",
    "        test_accuracy, test_loss = get_accuracy_and_loss_in_batches(X_test, y_test)\n",
    "#         valid_accuracy, valid_loss = get_accuracy_and_loss_in_batches(X_valid, y_valid)\n",
    "        # Save model weights for future use.\n",
    "        saved_model_path = saver.save(session, paths.model_path)\n",
    "\n",
    "        np.savez(paths.train_history_path, train_loss_history = train_loss_history, train_accuracy_history = train_accuracy_history, valid_loss_history = valid_loss_history, valid_accuracy_history = valid_accuracy_history)\n",
    "    \n",
    "        \n",
    "#         plot_learning_curves(params)\n",
    "#         pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = Parameters(\n",
    "    # Data parameters\n",
    "    num_classes = NUM_CLASS,\n",
    "    image_size = (32, 32),\n",
    "    # Training parameters\n",
    "    batch_size = 256,\n",
    "    max_epochs = 1,\n",
    "    log_epoch = 1,\n",
    "    print_epoch = 100,\n",
    "    # Optimisations\n",
    "    learning_rate_decay = False,\n",
    "    learning_rate = 0.0001,\n",
    "    l2_reg_enabled = True,\n",
    "    l2_lambda = 0.0001,\n",
    "    early_stopping_enabled = True,\n",
    "    early_stopping_patience = 100,\n",
    "    resume_training = True,\n",
    "    # Layers architecture\n",
    "    conv1_k = 5, conv1_d = 32, conv1_p = 0.9,\n",
    "    conv2_k = 5, conv2_d = 64, conv2_p = 0.8,\n",
    "    conv3_k = 5, conv3_d = 128, conv3_p = 0.7,\n",
    "    fc4_size = 1024, fc4_p = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/tuyenlv17/data/programming/projects/digital-race-2/GSS-CNN-classification/models/k5d32p0.9_k5d64p0.8_k5d128p0.7_fc1024p0.5_no-lrdec_l2/model.ckpt\n",
      "================= TRAINING ==================\n",
      "current epoch  0\n",
      "batch  1\n",
      "batch  2\n",
      "train accuracy  41.45383104125737  train loss  1.9400939\n",
      "valid accuracy  49.12280701754386  valid loss  1.9071995\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-fba8a2fe7be4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-124-51e089e05b62>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(params, X_train, y_train, X_valid, y_valid, X_test, y_test)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Evaluate on test dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy_and_loss_in_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;31m#         valid_accuracy, valid_loss = get_accuracy_and_loss_in_batches(X_valid, y_valid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# Save model weights for future use.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-124-51e089e05b62>\u001b[0m in \u001b[0;36mget_accuracy_and_loss_in_batches\u001b[0;34m(X, y, is_test)\u001b[0m\n\u001b[1;32m     52\u001b[0m                         \u001b[0mtf_x_batch\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                         \u001b[0mtf_y_batch\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                         \u001b[0mis_training\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                     }\n\u001b[1;32m     56\u001b[0m                 )\n",
      "\u001b[0;32m/opt/python-virtualenv/ml-python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/python-virtualenv/ml-python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/python-virtualenv/ml-python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/python-virtualenv/ml-python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/python-virtualenv/ml-python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(parameters,X_train, y_train, X_valid, y_valid, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
